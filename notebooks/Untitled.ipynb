{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcffff2e-bdce-42d0-a41d-afcfc37bab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a34775-9d0d-4d4b-a231-616f73dc8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\").to(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=-1  # -1 for CPU\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e90280-88f7-4ff1-8e85-c5099ef273fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartForSequenceClassification(\n",
      "  (model): BartModel(\n",
      "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartEncoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartDecoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): BartClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "BartTokenizerFast(name_or_path='facebook/bart-large-mnli', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267ac53-a729-4d06-a24d-5606147b3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Define the categories for classification\n",
    "    categories = [\n",
    "        \"Staff Professionalism\",\n",
    "        \"Communication Effectiveness\",\n",
    "        \"Appointment Availability\",\n",
    "        \"Waiting Time\",\n",
    "        \"Facility Cleanliness\",\n",
    "        \"Patient Respect\",\n",
    "        \"Treatment Quality\",\n",
    "        \"Staff Empathy and Compassion\",\n",
    "        \"Administrative Efficiency\",\n",
    "        \"Reception Staff Interaction\",\n",
    "        \"Environment and Ambiance\",\n",
    "        \"Follow-up and Continuity of Care\",\n",
    "        \"Accessibility and Convenience\",\n",
    "        \"Patient Education and Information\",\n",
    "        \"Feedback and Complaints Handling\",\n",
    "        \"Test Results\",\n",
    "        \"Surgery Website\",\n",
    "        \"Telehealth\",\n",
    "        \"Vaccinations\",\n",
    "        \"Prescriptions and Medication Management\",\n",
    "        \"Mental Health Support\",\n",
    "    ]\n",
    "\n",
    "    # Initialize the list to store labels\n",
    "    feedback_labels = [\"\"] * len(data)  # Pre-fill with empty strings\n",
    "\n",
    "    # Process batches\n",
    "    total_batches = (len(data) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "    for batch, start_index in tqdm(batch_generator(data, \"free_text\", batch_size), total=total_batches, desc=\"Processing batches\"):\n",
    "        # Validate and filter batch data\n",
    "        valid_sentences = [\n",
    "            (sentence.strip(), idx)\n",
    "            for idx, sentence in enumerate(batch)\n",
    "            if isinstance(sentence, str) and sentence.strip()\n",
    "        ]\n",
    "        if not valid_sentences:\n",
    "            continue  # Skip if no valid sentences are present\n",
    "\n",
    "        sentences, valid_indices = zip(*valid_sentences) if valid_sentences else ([], [])\n",
    "\n",
    "        try:\n",
    "            # Perform classification\n",
    "            model_outputs = classifier(list(sentences), categories)\n",
    "            # Assign the most relevant category label\n",
    "            for output, idx in zip(model_outputs, valid_indices):\n",
    "                feedback_labels[start_index + idx] = output[\"labels\"][0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during classification: {e}\")\n",
    "            # Optionally, handle specific actions for failed classification, such as logging or retrying\n",
    "\n",
    "    # Assign the computed labels back to the data\n",
    "    data[\"feedback_labels\"] = feedback_labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422abcb-c497-4dce-b0c9-32e0943d432a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
